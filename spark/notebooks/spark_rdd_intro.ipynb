{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:18pt; padding-top:20px; text-align:center\"><b>Introduction to <span style=\"font-weight:bold; color:green\">Spark</span></b></div><hr>\n",
    "<div style=\"text-align:right;\">Sergei Yu. Papulin <span style=\"font-style: italic;font-weight: bold;\">(papulin_bmstu@mail.ru)</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"0\"></a>\n",
    "<div><span style=\"font-size:14pt; font-weight:bold\">Contents</span>\n",
    "    <ol>\n",
    "        <li><a href=\"#1\">Word Count</a>\n",
    "            <ol style = \"list-style-type:lower-alpha\">\n",
    "                <li><a href=\"#1a\">Java</a></li>\n",
    "                <li><a href=\"#1b\">Python</a></li>\n",
    "                <li><a href=\"#1c\">Scala</a></li>\n",
    "            </ol>\n",
    "        </li>\n",
    "        <li><a href=\"#2\">Average Rating Calculation</a>\n",
    "            <ol style = \"list-style-type:lower-alpha\">\n",
    "                <li><a href=\"#2a\">Average ratings for each product</a></li>\n",
    "                <li><a href=\"#2b\">Average rating of all products</a></li>\n",
    "                <li><a href=\"#2c\">Filter items by their ratings</a></li>\n",
    "                <li><a href=\"#2d\">Average rating of product</a></li>\n",
    "            </ol>\n",
    "        </li>\n",
    "        <li><a href=\"#3\">Spark on AWS EMR</a></li>\n",
    "        <li><a href=\"#4\">References</a></li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Launch the cell below to apply a jupyter notebook style</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<link href=\"css/style.css\" rel=\"stylesheet\" type=\"text/css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:14pt; font-weight:bold\">1. Word Count</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To Content</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1a\"></a>\n",
    "<div style=\"display:table; width:100%\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-style:italic; font-weight:bold; font-size:12pt\">\n",
    "            a. Java\n",
    "        </div>\n",
    "        <div style=\"display:table-cell; border:1px solid lightgrey; width:20%\">\n",
    "            <div style=\"display:table-cell; width:10%; text-align:center; background-color:whitesmoke;\">\n",
    "                <a href=\"#1\">Back</a>\n",
    "            </div>\n",
    "            <div style=\"display:table-cell; width:10%; text-align:center;\">\n",
    "                <a href=\"#1b\">Next</a>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Preparation to launch on the local Cloudera VM</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Connect to your local Cloudera VM via SSH</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo ssh -p 2222 cloudera@127.0.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Create a directory to store data and spark files locally</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /home/cloudera/classes/spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Create a directory in HDFS for data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -mkdir -p /data/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Copy the 100-sample dataset from your local node to the Cloudera VM</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo scp -P 2222 /YOUR_PATH/data/samples_100.json cloudera@127.0.0.1:/home/cloudera/classes/spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Move the dataset to HDFS</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -moveFromLocal /home/cloudera/classes/spark/samples_100.json hdfs:///data/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Run and debug a Spark code in IntelliJ IDE</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Java code of word count example</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>JAVA 7</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Add to your project the following libraries</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark:spark-core_2.10:1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.json:json:20171018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package edu.classes.spark;\n",
    "\n",
    "import org.apache.spark.api.java.JavaSparkContext;\n",
    "import org.apache.spark.api.java.JavaRDD;\n",
    "import org.apache.spark.api.java.JavaPairRDD;\n",
    "import org.apache.spark.SparkConf;\n",
    "\n",
    "import org.apache.spark.api.java.function.FlatMapFunction;\n",
    "import org.apache.spark.api.java.function.Function2;\n",
    "import org.apache.spark.api.java.function.PairFunction;\n",
    "import scala.Tuple2;\n",
    "\n",
    "import org.json.JSONObject;\n",
    "\n",
    "import java.util.Arrays;\n",
    "\n",
    "\n",
    "class SplitReviewByWords implements FlatMapFunction<String, String> {\n",
    "    public Iterable<String> call(String strReviewJSON) {\n",
    "\n",
    "        JSONObject reviewJSON = new JSONObject(strReviewJSON);\n",
    "\n",
    "        return Arrays.asList(reviewJSON.getString(\"reviewText\").split(\" \"));\n",
    "    }\n",
    "}\n",
    "\n",
    "class MapToWordTuple implements PairFunction<String, String, Integer> {\n",
    "\n",
    "    public Tuple2<String, Integer> call(String word) {\n",
    "\n",
    "        return new Tuple2<>(word, 1);\n",
    "    }\n",
    "}\n",
    "\n",
    "class ReduceCountWords implements Function2<Integer, Integer, Integer> {\n",
    "\n",
    "    public Integer call(Integer v1, Integer v2) {\n",
    "\n",
    "        return v1 + v2;\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "public class WordCount {\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "\n",
    "        SparkConf conf = new SparkConf().setAppName(\"SparkJavaWordCount\").setMaster(\"local[2]\");\n",
    "        JavaSparkContext sc = new JavaSparkContext(conf);\n",
    "\n",
    "        JavaRDD<String> textFile = sc.textFile(args[0]);\n",
    "\n",
    "        JavaRDD<String> words = textFile.flatMap(new SplitReviewByWords());\n",
    "\n",
    "        JavaPairRDD<String, Integer> wordTuple = words.mapToPair(new MapToWordTuple());\n",
    "\n",
    "        JavaPairRDD<String, Integer> wordCount = wordTuple.reduceByKey(new ReduceCountWords());\n",
    "\n",
    "        //System.out.println(wordCount.collect());\n",
    "\n",
    "        wordCount.saveAsTextFile(args[1]);\n",
    "\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>JAVA 8</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark:spark-core_2.11:2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.json:json:20180130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package edu.classes.spark;\n",
    "\n",
    "import org.apache.spark.SparkConf;\n",
    "import org.apache.spark.api.java.JavaPairRDD;\n",
    "import org.apache.spark.api.java.JavaRDD;\n",
    "import org.apache.spark.api.java.JavaSparkContext;\n",
    "import org.json.JSONObject;\n",
    "import scala.Tuple2;\n",
    "\n",
    "import java.util.Arrays;\n",
    "\n",
    "\n",
    "public class WordCount {\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "\n",
    "        SparkConf conf = new SparkConf().setAppName(\"SparkJava8WordCount\").setMaster(\"local[2]\");\n",
    "        JavaSparkContext sc = new JavaSparkContext(conf);\n",
    "\n",
    "        JavaRDD<String> textFile = sc.textFile(args[0]);\n",
    "\n",
    "        JavaRDD<String> reviews = textFile.map(row -> new JSONObject(row).getString(\"reviewText\"));\n",
    "\n",
    "        JavaRDD<String> words = reviews.flatMap(review -> Arrays.asList(review.split(\" \")).iterator());\n",
    "\n",
    "        JavaPairRDD<String, Integer> wordTuple = words.mapToPair(word -> new Tuple2<>(word, 1));\n",
    "\n",
    "        JavaPairRDD<String, Integer> wordCount = wordTuple.reduceByKey((x, y) -> x + y);\n",
    "\n",
    "        //System.out.println(wordCount.collect());\n",
    "\n",
    "        wordCount.saveAsTextFile(args[1]);\n",
    "\n",
    "    }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Short form</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JavaPairRDD<String, Integer> wordCount = textFile\n",
    "        .map(row -> new JSONObject(row).getString(\"reviewText\"))\n",
    "        .flatMap(review -> Arrays.asList(review.split(\" \")).iterator())\n",
    "        .mapToPair(word -> new Tuple2<>(word, 1))\n",
    "        .reduceByKey((x, y) -> x + y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Create a jar file to execute the word count Spark Application on the local Cloudera VM. But before that, remove \".setMaster(\"local[2]\")\" from the above code</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Copy the jar file to the Cloudera VM</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo scp -P 2222 /YOUR_PATH/SparkWordCount.jar cloudera@127.0.0.1:/home/cloudera/classes/spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Connect to the Cloudera VM via SSH and run the Spark Application</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit --master yarn /home/cloudera/classes/spark/SparkWordCount.jar hdfs:///data/input/samples_100.json  hdfs:///data/output_spark_java_wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Print out the result</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cat /data/output_spark_java_wordcount/par*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Remove the output directory (required for re-running the application)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -rm -r /data/output_spark_java_wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1b\"></a>\n",
    "<div style=\"display:table; width:100%\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-style:italic; font-weight:bold; font-size:12pt\">\n",
    "            b. Python\n",
    "        </div>\n",
    "        <div style=\"display:table-cell; border:1px solid lightgrey; width:20%\">\n",
    "            <div style=\"display:table-cell; width:10%; text-align:center; background-color:whitesmoke;\">\n",
    "                <a href=\"#1a\">Back</a>\n",
    "            </div>\n",
    "            <div style=\"display:table-cell; width:10%; text-align:center;\">\n",
    "                <a href=\"#1c\">Next</a>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import json\n",
    "\n",
    "conf = SparkConf().setAppName(\"SparkPythonWordCount\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "textFile = sc.textFile(sys.argv[1])\n",
    "\n",
    "def split_review(review_json_item):\n",
    "    dict_review_item = json.loads(review_json_item)\n",
    "    return dict_review_item[\"reviewText\"].split(\" \")\n",
    "\n",
    "wordCount = textFile.flatMap(lambda row: split_review) \\\n",
    "                          .map(lambda word: (word, 1)) \\\n",
    "                          .reduceByKey(lambda v1, v2: v1 + v2)\n",
    "        \n",
    "wordCount.saveAsTextFile(sys.argv[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1c\"></a>\n",
    "<div style=\"display:table; width:100%\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-style:italic; font-weight:bold; font-size:12pt\">\n",
    "            c. Scala\n",
    "        </div>\n",
    "        <div style=\"display:table-cell; border:1px solid lightgrey; width:20%\">\n",
    "            <div style=\"display:table-cell; width:10%; text-align:center; background-color:whitesmoke;\">\n",
    "                <a href=\"#1b\">Back</a>\n",
    "            </div>\n",
    "            <div style=\"display:table-cell; width:10%; text-align:center;\">\n",
    "                <a href=\"#2\">Next</a>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>build.sbt</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name := \"SparkScalaWordCount\"\n",
    "\n",
    "version := \"0.1\"\n",
    "\n",
    "scalaVersion := \"2.11.12\"\n",
    "\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.3.0\"\n",
    "libraryDependencies += \"org.json\" % \"json\" % \"20180130\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Word count scala code</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package edu.classes.spark\n",
    "\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.json.JSONObject\n",
    "\n",
    "object WordCount {\n",
    "\n",
    "  def main(args: Array[String]): Unit = {\n",
    "\n",
    "    val conf = new SparkConf().setAppName(\"SparkScalaWordCount\").setMaster(\"local[2]\")\n",
    "    val sc = new SparkContext(conf)\n",
    "\n",
    "    val textFile = sc.textFile(args(0))\n",
    "\n",
    "    val reviews = textFile.map(row => new JSONObject(row).getString(\"reviewText\"))\n",
    "\n",
    "    val words = reviews.flatMap(review => review.split(\" \"))\n",
    "\n",
    "    val wordTuple = words.map(word => (word, 1))\n",
    "\n",
    "    val wordCount = wordTuple.reduceByKey((x, y) => x + y)\n",
    "\n",
    "    wordCount.take(5).foreach(println)\n",
    "\n",
    "    // wordCount.saveAsTextFile(args(1))\n",
    "\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Short form</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wordCount = textFile\n",
    "  .map(row => new JSONObject(row).getString(\"reviewText\"))\n",
    "  .flatMap(review => review.split(\" \"))\n",
    "  .map(word => (word, 1))\n",
    "  .reduceByKey((x, y) => x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:14pt; font-weight:bold\">2. Average Rating Calculation</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To Content</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2a\"></a>\n",
    "<div style=\"display:table; width:100%\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-style:italic; font-weight:bold; font-size:12pt\">\n",
    "            a. Average ratings for each product\n",
    "        </div>\n",
    "        <div style=\"display:table-cell; border:1px solid lightgrey; width:20%\">\n",
    "            <div style=\"display:table-cell; width:10%; text-align:center; background-color:whitesmoke;\">\n",
    "                <a href=\"#2\">Back</a>\n",
    "            </div>\n",
    "            <div style=\"display:table-cell; width:10%; text-align:center;\">\n",
    "                <a href=\"#2b\">Next</a>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Run a Spark Application interactively in <span class=\"code-font code-key\">pyspark</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>[OPTIONAL] Connect to your local Cloudera VM via SSH </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo ssh -p 2222 cloudera@127.0.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Copy sample reviews to HDFS from local file system (in terminal)</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -mkdir -p data/spark_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -copyFromLocal /YOUR_PATH/data/spark-rdd-intro/samples_100.json data/spark_rdd/samples_100.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls data/spark_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Run <span class=\"code-font code-key\">pyspark</span> in command line and execute the following commands in sequence</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/spark_rdd/samples_100.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RDD from the dataset in HDFS\n",
    "rdd_review_100 = sc.textFile(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Print out single review</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_review_100.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Write a function to extract a product ID and rating from a json structure of a product item</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prod_rating(review_json_item):\n",
    "    dict_review_item = json.loads(review_json_item)\n",
    "    return (dict_review_item[\"asin\"], dict_review_item[\"overall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Create RDD with a product ID and rating for each product</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_prod_rating = rdd_review_100.map(lambda row: get_prod_rating(row))\n",
    "rdd_prod_rating.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Create RDD with average ratings of products</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_prod_rating.aggregateByKey((0,0), lambda x, value: (x[0] + value, x[1] + 1), \n",
    "                               lambda x, y: (x[0] + y[0], x[1] + y[1])).mapValues(lambda x: x[0]/x[1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Run a Spark Application with <span class=\"code-font code-key\">spark-submit</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Create a python file with the following content</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import json\n",
    "\n",
    "conf = SparkConf().setAppName(\"AvgRatingByProd\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "rdd_review = sc.textFile(sys.argv[1])\n",
    "\n",
    "def get_prod_rating(review_json_item):\n",
    "    dict_review_item = json.loads(review_json_item)\n",
    "    return (dict_review_item[\"asin\"], dict_review_item[\"overall\"])\n",
    "\n",
    "rdd_prod_rating = rdd_review.map(lambda row: get_prod_rating(row))\n",
    "rdd_avg_by_prod = rdd_prod_rating.aggregateByKey((0,0), \n",
    "                                        lambda x, value: (x[0] + value, x[1] + 1), \n",
    "                                        lambda x, y: (x[0] + y[0], x[1] + y[1])).mapValues(lambda x: x[0]/x[1])\n",
    "\n",
    "rdd_avg_by_prod.saveAsTextFile(sys.argv[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>[OPTIONAL] Copy the python file to the Cloudera VM</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo scp -P 2222 /YOUR_PATH/avg_rating_by_prod.py cloudera@127.0.0.1:/home/cloudera/classes/spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>[OPTIONAL] Connect to the Cloudera VM via SSH</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Run the Spark Application</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit --master yarn /YOUR_PATH/avg_rating_by_prod.py hdfs:///data/input/samples_100.json  hdfs:///data/output_spark_avg_rating_by_prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Check the output directory in HDFS</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /data/output_spark_avg_rating_by_prod/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Print out the result</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cat /data/output_spark_avg_rating_by_prod/part-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Remove the output directory (required for re-running the application)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -rm -r /data/output_spark_avg_rating_by_prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>[OPTIONAL] <b>Open Spark UI to monitor Spark applications</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.cloudera.com/documentation/enterprise/5-8-x/topics/admin_spark_history_server.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Add the following lines to <span class=\"code-font\">/etc/spark/conf/spark-defaults.conf</span> in the Cloudera VM</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.eventLog.dir=hdfs:///user/spark/applicationHistory\n",
    "spark.eventLog.enabled=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>[OPTIONAL] Use port forwarding from your local host to the local Cloudera VM to access a Spark History Server</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo ssh -N -f -L 9964:quickstart.cloudera:18088 cloudera@127.0.0.1 -p 2222"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Open a web browser to see a Hadoop dashboard</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"code-block code-font\"><a href=\"http://quickstart.cloudera:18088/\">http://quickstart.cloudera:18088/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>[OPTIONAL] or with port forwarding</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"code-block code-font\"><a href=\"http://localhost:9964\">http://localhost:9964</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2b\"></a>\n",
    "<div style=\"display:table; width:100%\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-style:italic; font-weight:bold; font-size:12pt\">\n",
    "            b. Average rating of all products\n",
    "        </div>\n",
    "        <div style=\"display:table-cell; border:1px solid lightgrey; width:20%\">\n",
    "            <div style=\"display:table-cell; width:10%; text-align:center; background-color:whitesmoke;\">\n",
    "                <a href=\"#2a\">Back</a>\n",
    "            </div>\n",
    "            <div style=\"display:table-cell; width:10%; text-align:center;\">\n",
    "                <a href=\"#2c\">Next</a>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p>Create a python file with the following content</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import json\n",
    "\n",
    "conf = SparkConf().setAppName(\"AvgRating\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "rdd_review = sc.textFile(sys.argv[1])\n",
    "\n",
    "def get_prod_rating(review_json_item):\n",
    "    rating = json.loads(review_json_item)[\"overall\"]\n",
    "    if isinstance(rating, float):\n",
    "        return rating\n",
    "    return None\n",
    "\n",
    "rdd_prod_rating = rdd_review.map(lambda row: get_prod_rating(row)).filter(lambda rating: rating is not None)\n",
    "\n",
    "rating_count = rdd_prod_rating.aggregate((0,0), \n",
    "                                       lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                                       lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "avg_rating = sc.parallelize([rating_count[0] / rating_count[1]])\n",
    "\n",
    "avg_rating.saveAsTextFile(sys.argv[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Copy the python file to the Cloudera VM and run using the <span class=\"code-font code-key\">spark-submit</span> command</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit --master yarn /home/cloudera/classes/spark/avg_rating.py hdfs:///data/input/samples_100.json  hdfs:///data/output_spark_avg_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Print out the result</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cat /data/output_spark_avg_rating/part-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Remove the output directory (required for re-running the application)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -rm -r /data/output_spark_avg_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2c\"></a>\n",
    "<div style=\"display:table; width:100%\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-style:italic; font-weight:bold; font-size:12pt\">\n",
    "            c. Filter items by their ratings\n",
    "        </div>\n",
    "        <div style=\"display:table-cell; border:1px solid lightgrey; width:20%\">\n",
    "            <div style=\"display:table-cell; width:10%; text-align:center; background-color:whitesmoke;\">\n",
    "                <a href=\"#2b\">Back</a>\n",
    "            </div>\n",
    "            <div style=\"display:table-cell; width:10%; text-align:center;\">\n",
    "                <a href=\"#2d\">Next</a>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Create a python file with the following content</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import json\n",
    "\n",
    "def check_and_return_rating_arg(rating):\n",
    "    try:\n",
    "        return float(rating) \n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "# Global variable\n",
    "rating_threshold = check_and_return_rating_arg(sys.argv[3])\n",
    "\n",
    "rdd_review = sc.textFile(sys.argv[1])\n",
    "\n",
    "def filter_by_rating(review_json_item):\n",
    "    rating = json.loads(review_json_item)[\"overall\"]\n",
    "    if isinstance(rating, float) and rating >= rating_threshold:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "rdd_items = rdd_review.filter(lambda row: filter_by_rating(row))\n",
    "rdd_items.saveAsTextFile(sys.argv[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Copy the python file to the Cloudera VM</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo scp -P 2222 /YOUR_PATH/filter_by_threshold.py cloudera@127.0.0.1:/home/cloudera/classes/spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Connect to the Cloudera VM via SSH and run the Spark Application</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit --master yarn /home/cloudera/classes/spark/filter_by_threshold.py  hdfs:///data/input/samples_100.json  hdfs:///data/output_spark_filter_by_threshold 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Print out the result</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cat /data/output_spark_filter_by_threshold/part-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Remove the output directory (required for re-running the application)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -rm -r /data/output_spark_filter_by_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:14pt; font-weight:bold\">3. Spark on AWS EMR\n",
    "</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To Content</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"msg-block msg-task\">\n",
    "  <div class=\"msg-text-task\"><p>Deploy an EMR cluser with 3 worker instances and run the apps from the previous section</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To launch a Spark EMR Cluster, use the following command</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws emr create-cluster \\\n",
    "    --name \"Spark_Cluster\" \\\n",
    "    --release-label emr-5.8.0 \\\n",
    "    --applications Name=Spark Name=Zeppelin \\\n",
    "    --log-uri s3://YOUR_BUCKET/logs/ \\\n",
    "    --service-role emr-default-role \\\n",
    "    --instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m4.large InstanceGroupType=CORE,InstanceCount=3,InstanceType=m4.large \\\n",
    "    --ec2-attributes InstanceProfile=emr-default-ec2-role,KeyName=YOUR_KEYS,SubnetId=\"YOUR_SUBNET\" \\\n",
    "    --configurations file:///YOUR_PATH/config/hdfs-config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR INSTRUCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:14pt; font-weight:bold\">4. References</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To content</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a href=\"https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html\">Spark Programming Guide</a><br>\n",
    "<a href=\"https://spark.apache.org/docs/latest/submitting-applications.html\">Submitting Applications</a><br>\n",
    "<a href=\"https://hortonworks.com/tutorial/setting-up-a-spark-development-environment-with-java/\">Setting up a Spark Development Environment with Java</a><br>\n",
    "<a href=\"https://hortonworks.com/tutorial/setting-up-a-spark-development-environment-with-scala/\">Setting up a Spark Development Environment with Scala</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
