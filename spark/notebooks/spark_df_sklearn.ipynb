{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark + sklearn\n",
    "\n",
    "---\n",
    "\n",
    "S.Yu. Papulin (papulin_bmstu@mail.ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [Prediction for DataFrame](#Prediction-for-DataFrame)\n",
    "    - [Using UDF](#Using-UDF)\n",
    "    - [Using Pandas UDF](#Using-Pandas-UDF)\n",
    "- [Model Selection](#Model-Selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn related packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark related packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ubuntu/BigData/spark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/home/ubuntu/ML/anaconda3/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/home/ubuntu/ML/anaconda3/bin/python\"\n",
    "\n",
    "spark_home = os.environ.get(\"SPARK_HOME\")\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python\"))\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python/lib/py4j-0.10.7-src.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    FloatType,\n",
    "    ArrayType,\n",
    "    StringType\n",
    ")\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, pandas_udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка исходных данных\n",
    "housing = datasets.fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing.data[:,:6]\n",
    "y = housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLMS = housing.feature_names[:6] + [\"MEDV\"]\n",
    "CLMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализация признаков\n",
    "X = ((X - X.mean(axis=0)) / X.std(axis=0)).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на обучающее и тестовое множества\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# mae_linear = mean_absolute_error(y_test, model.predict(X_test))\n",
    "# print(\"MAE =\", mae_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf()\\\n",
    "        .setAppName(\"bostonApp\")\\\n",
    "        .setMaster(\"local[*]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"bostonApp\")\\\n",
    "    .config(conf=conf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLMS = housing.feature_names[:6] + [\"MEDV\"]\n",
    "pdf = pd.DataFrame(data=np.c_[X_test, y_test], columns=CLMS)\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pdf)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_broadcast = spark.sparkContext.broadcast(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=FloatType())\n",
    "def predict_price(*row):\n",
    "    model = model_broadcast.value\n",
    "    return float(model.predict([row,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLMNS_ = CLMS.copy()\n",
    "CLMNS_.remove(\"MEDV\")\n",
    "clmns = [F.col(clmn) for clmn in CLMNS_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted = df.withColumn(\"MEDV_PREDICT\", predict_price(*clmns))\n",
    "df_predicted.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_predicted = df_predicted.toPandas()\n",
    "pdf_predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mae_linear = mean_absolute_error(pdf_predicted[\"MEDV\"], pdf_predicted[\"MEDV_PREDICT\"])\n",
    "# print(\"MAE =\", mae_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ~/ML/anaconda3/bin/pip install pyarrow>=0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"float\", F.PandasUDFType.SCALAR)\n",
    "def predict_price_(*cols):\n",
    "    import pandas as pd\n",
    "    pdf = pd.concat(cols, axis=1, ignore_index=True)\n",
    "    model = model_broadcast.value\n",
    "    if not isinstance(model, LinearRegression):    \n",
    "        raise Exception(\"Not model.\")\n",
    "    return pd.Series(model.predict(pdf)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLMNS_ = CLMS.copy()\n",
    "CLMNS_.remove(\"MEDV\")\n",
    "clmns = [F.col(clmn) for clmn in CLMNS_]\n",
    "clmns = [clmn for clmn in CLMNS_]\n",
    "clmns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted = df.withColumn(\"MEDV_PREDICT\", predict_price_(*clmns))\n",
    "df_predicted.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_broadcast = spark.sparkContext.broadcast((X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    \"n_estimators\": np.arange(50, 300, 50), \n",
    "    \"max_depth\": np.arange(5, 21, 5),\n",
    "}\n",
    "\n",
    "grid = ParameterGrid(params_grid)\n",
    "list(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_grid = spark.sparkContext.parallelize(grid, 4)\n",
    "rdd_grid.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params_set):\n",
    "    from sklearn.ensemble import ExtraTreesRegressor\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X, y = data_broadcast.value\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE)\n",
    "    for params in params_set:\n",
    "        model = ExtraTreesRegressor(**params).fit(X_train, y_train)\n",
    "        yield model.score(X_test, y_test), params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_grid\\\n",
    "    .mapPartitions(train)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://spark.apache.org/docs/2.4.7/api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf\n",
    "https://spark.apache.org/docs/2.4.7/sql-pyspark-pandas-with-arrow.html\n",
    "https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
