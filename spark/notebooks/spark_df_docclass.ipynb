{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Document Classification using Spark\n",
    "\n",
    "---\n",
    "\n",
    "S.Yu. Papulin (papulin_bmstu@mail.ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [Multiclass Document Classification using Naive Bayes Multinomial Model](#Multiclass-Document-Classification-using-Naive-Bayes-Multinomial-Model)\n",
    "    - [Loading Dataset](#Loading-Dataset)\n",
    "    - [Creating Spark DataFrame](#Creating-Spark-DataFrame)\n",
    "    - [Splitting Dataset](#Splitting-Dataset)\n",
    "    - [Vectorizing Documents](#Vectorizing-Documents)\n",
    "    - [Training Model](#Training-Model)\n",
    "    - [Testing Model](#Testing-Model)\n",
    "[Pipelines for Classification](#Pipelines-for-Classification)\n",
    "    - [Training and Transforming with Pipeline](#Training-and-Transforming-with-Pipeline)\n",
    "    - [Adding New Transformation to Pipeline](#Adding-New-Transformation-to-Pipeline)\n",
    "- [Model Selection](#Model-Selection)\n",
    "    - [Parameter Grid](#Parameter-Grid)\n",
    "    - [Train-Validation Split](#Train-Validation-Split)\n",
    "    - [Cross-Validation](#Cross-Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OPTIONAL] Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"]=\"/home/ubuntu/BigData/spark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/home/ubuntu/ML/anaconda3/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/home/ubuntu/ML/anaconda3/bin/python\"\n",
    "\n",
    "spark_home = os.environ.get(\"SPARK_HOME\")\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python\"))\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python/lib/py4j-0.10.7-src.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `PySpark`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*On YARN:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = pyspark.SparkConf() \\\n",
    "#         .setAppName(\"docClassificationApp\") \\\n",
    "#         .setMaster(\"yarn\") \\\n",
    "#         .set(\"spark.submit.deployMode\", \"client\")\n",
    "\n",
    "# sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Locally:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: spark.executor.* options are not the case in the local mode \n",
    "#  as all computation happens in the driver.\n",
    "conf = pyspark.SparkConf()\\\n",
    "        .set(\"spark.executor.memory\", \"1g\")\\\n",
    "        .set(\"spark.executor.core\", \"2\")\\\n",
    "        .set(\"spark.driver.memory\", \"2g\")\\\n",
    "        .setMaster(\"local[*]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf=conf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Document Classification using Naive Bayes Multinomial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid additional preprocessing steps, let's get the 20newsgroups dataset from `scikit-learn` library. You can download the raw dataset from [here](http://qwone.com/~jason/20Newsgroups/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_20newsgroups = fetch_20newsgroups(\n",
    "    subset=\"all\", remove=[\"headers\", \"footer\", \"quotes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_20newsgroups.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_20newsgroups.data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_20newsgroups.target[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_20newsgroups.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data_20newsgroups.target_names[i] for i in data_20newsgroups.target[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spark `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_doc_target = zip(data_20newsgroups.data, data_20newsgroups.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Spark `DataFrame` for the document collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = spark.sparkContext.parallelize(pairs_doc_target, 4)\\\n",
    "    .map(lambda x: Row(document=x[0], target=int(x[1])))\\\n",
    "    .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.show(2, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = df_data.randomSplit([0.8, 0.2], seed=1234)\n",
    "df_train.persist().count(), df_test.persist().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer, \n",
    "    StopWordsRemover,\n",
    "    HashingTF, \n",
    "    IDF\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"document\", \n",
    "                                outputCol=\"tokens\", \n",
    "                                gaps=False,\n",
    "                                pattern=\"(?!_)[A-Za-z']+\")\n",
    "\n",
    "# regexTokenizer = Tokenizer(inputCol=\"document\", \n",
    "#                                 outputCol=\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train__tokens = regexTokenizer.transform(df_train)\n",
    "df_train__tokens\\\n",
    "    .select(\"tokens\")\\\n",
    "    .show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train__filtered = remover.transform(df_train__tokens)\n",
    "df_train__filtered\\\n",
    "    .select(\"tokens\", \"filtered\")\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"filtered\",\n",
    "                      outputCol=\"tf\", \n",
    "                      numFeatures=200000,\n",
    "                      binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train__tf = hashingTF.transform(df_train__filtered)\n",
    "df_train__tf\\\n",
    "    .select(\"tokens\", \"filtered\", \"tf\")\\\n",
    "    .show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"tf\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df_train__tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train__tf_idf = idf_model.transform(df_train__tf)\n",
    "df_train__tf_idf\\\n",
    "    .select(\"tf\", \"features\")\\\n",
    "    .show(2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(labelCol=\"target\", \n",
    "                featuresCol=\"features\", \n",
    "                smoothing=1.0, \n",
    "                modelType=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nb.fit(df_train__tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train__predictions = model.transform(df_train__tf_idf)\n",
    "df_train__predictions\\\n",
    "    .select(\"probability\", \"target\", \"prediction\")\\\n",
    "    .show(1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", \n",
    "                                              predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = evaluator.evaluate(df_train__predictions)\n",
    "print(\"Train accuracy = \" + str(train_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test__tokens = regexTokenizer.transform(df_test)\n",
    "df_test__filtered = remover.transform(df_test__tokens)\n",
    "df_test__tf = hashingTF.transform(df_test__filtered)\n",
    "df_test__tf_idf = idf_model.transform(df_test__tf)\n",
    "df_test__predictions = model.transform(df_test__tf_idf)\n",
    "test_accuracy = evaluator.evaluate(df_test__predictions)\n",
    "print(\"Test accuracy = \" + str(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred_count = df_test__predictions.select(\"target\", \"prediction\")\\\n",
    "    .groupBy(\"target\", \"prediction\")\\\n",
    "    .count()\\\n",
    "    .toPandas()\n",
    "\n",
    "true_pred_count.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(true_pred_count, \n",
    "                          classes=None,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues, \n",
    "                          figsize=(8,8)):\n",
    "    \"\"\"\n",
    "    Plotting Confusion Matrix\n",
    "    \n",
    "    Note: The code from [here](1) was adapted to use a Pandas DataFrame with the following structure:\n",
    "         (index, true_value, predicted_value, count)\n",
    "    \n",
    "    Refs:\n",
    "    [1] https://scikit-learn.org/0.21/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.utils.multiclass import unique_labels\n",
    "    \n",
    "    classes = classes if classes else unique_labels(true_pred_count[\"target\"], true_pred_count[\"prediction\"])\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = np.zeros((len(classes), len(classes)), dtype=\"int\")\n",
    "\n",
    "    for indx, row in true_pred_count.iterrows():\n",
    "        cm[int(row[\"target\"]), int(row[\"prediction\"])] = row[\"count\"]\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    plt.ylim(len(classes)-0.5, -0.5)\n",
    "    \n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(true_pred_count, data_20newsgroups.target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Transforming with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        regexTokenizer, \n",
    "        remover, \n",
    "        hashingTF.setInputCol(\"filtered\"), \n",
    "        idf_model, \n",
    "        nb\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model__pipelined = pipeline.fit(df_train)\n",
    "model__pipelined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test__predictions = model__pipelined.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = evaluator.evaluate(df_test__predictions)\n",
    "print(\"Test accuracy = \" + str(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding New Transformation to Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = NGram(n=2, inputCol=\"filtered\", outputCol=\"ngrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ngram = Pipeline(stages=[\n",
    "    regexTokenizer, \n",
    "    remover, \n",
    "    ngram,\n",
    "    hashingTF.setInputCol(\"ngrams\"), \n",
    "    idfModel, \n",
    "    nb\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model__pipelined_ngram = pipeline_ngram.fit(df_train)\n",
    "df_test__predictions = model__pipelined_ngram.transform(df_test)\n",
    "test_accuracy = evaluator.evaluate(df_test__predictions)\n",
    "print(\"Test accuracy = \" + str(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying New Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_document = [\n",
    "    \"Victory means Jurgen Klopp's side is now unbeaten in its last 64 league games at home \"\n",
    "    \"-- a run that stretches back to May 2017. The previous record of 63 was set by Bob Paisley's \"\n",
    "    \"team between 1978 and 1981 and was ended by Leicester City. However, history did not repeat \"\n",
    "    \"itself at the weekend and Liverpool was a deserved winner against the Foxes, producing \"\n",
    "    \"a comprehensive display against one of the most dangerous sides in the English Premier League.\"]\n",
    "\n",
    "df_new_document = spark.sparkContext.parallelize(new_document, 1) \\\n",
    "    .map(lambda x: Row(document=x)) \\\n",
    "    .toDF()\n",
    "\n",
    "df_new_document.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test__predictions = model__pipelined.transform(df_new_document)\n",
    "new_document__prediction = int(df_test__predictions.select(\"prediction\").collect()[0][\"prediction\"])\n",
    "data_20newsgroups.target_names[new_document__prediction]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import (\n",
    "    ParamGridBuilder, \n",
    "    TrainValidationSplit, \n",
    "    CrossValidator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"filtered\",\n",
    "                      outputCol=\"tf\",\n",
    "                      binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of features\n",
    "num_features_list = [20000, 200000]\n",
    "\n",
    "# The smooth parameter of Naive Bayes\n",
    "nb_smoothing_list = [0.1, 1.0]\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, num_features_list) \\\n",
    "    .addGrid(nb.smoothing, nb_smoothing_list) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        regexTokenizer, \n",
    "        remover, \n",
    "        hashingTF, \n",
    "        idf_model, \n",
    "        nb\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=evaluator,\n",
    "                           trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_model = tvs.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_model.validationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model__best = split_model.bestModel\n",
    "model__best.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model__best.transform(df_test) \\\n",
    "    .select(\"features\", \"target\", \"prediction\") \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test__predictions = model__best.transform(df_test)\n",
    "test_accuracy = evaluator.evaluate(df_test__predictions)\n",
    "print(\"Test accuracy = {}\".format(str(test_accuracy)))\n",
    "print(\"Best model parameters:\")\n",
    "print(\"\\tNB Smoothing = {}\".format(model__best.stages[-1].getOrDefault(\"smoothing\")))\n",
    "print(\"\\tNumber of features = {}\".format(model__best.stages[2].getNumFeatures()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = crossval.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model__best = cv_model.bestModel\n",
    "model__best.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test__predictions = model__best.transform(df_test)\n",
    "test_accuracy = evaluator.evaluate(df_test__predictions)\n",
    "print(\"Test accuracy = {}\".format(str(test_accuracy)))\n",
    "print(\"Best model parameters:\")\n",
    "print(\"\\tNB Smoothing = {}\".format(model__best.stages[-1].getOrDefault(\"smoothing\")))\n",
    "print(\"\\tNumber of features = {}\".format(model__best.stages[2].getNumFeatures()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Machine Learning Library (MLlib) Guide](http://spark.apache.org/docs/latest/ml-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
