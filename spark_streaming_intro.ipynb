{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:18pt; padding-top:20px; text-align:center\"><b>Introduction to </b><span style=\"font-weight:bold; color:green\">Spark Streaming</span></div><hr>\n",
    "<div style=\"text-align:right;\">Папулин С.Ю. <span style=\"font-style: italic;font-weight: bold;\">(papulin.study@yandex.ru)</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"0\"></a>\n",
    "<div><span style=\"font-size:14pt; font-weight:bold\">Contents</span>\n",
    "    <ol>\n",
    "        <li><a href=\"#1\">Stateless Transformation</a></li>\n",
    "        <li><a href=\"#2\">Stateful Transformation</a></li>\n",
    "        <li><a href=\"#3\">Window Transformation</a></li>\n",
    "        <li><a href=\"#4\">Sources</a></li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:14pt; font-weight:bold\">1. Stateless Transformation</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To contents</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !!! COPY CONTENT OF THIS CELL AND PAST IT INTO SEPERATE PY FILE TO RUN IN TERMINAL !!!\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\n",
    "\n",
    "# Create Spark Context\n",
    "sc = SparkContext(appName=\"WordCount\")\n",
    "\n",
    "# Set log level\n",
    "#sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Batch interval (10 seconds)\n",
    "batch_interval = 10\n",
    "\n",
    "# Create Streaming Context\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "# Create a stream (DStream)\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "\n",
    "# TRANSFORMATION FOR EACH BATCH\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    MapFlat Transformation\n",
    "\n",
    "    Example: [\"a a b\", \"b c\"] => [\"a\", \"a\", \"b\", \"b\", \"c\"] \n",
    "\n",
    "\"\"\"\n",
    "words = lines.flatMap(lambda line: line.split())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Map Transformation\n",
    "\n",
    "    Example: [\"a\", \"a\", \"b\", \"b\", \"c\"] => [(\"a\",1), (\"a\",1), (\"b\",1), (\"b\",1), (\"c\",1)] ] \n",
    "\n",
    "\"\"\"\n",
    "word_tuples = words.map(lambda word: (word, 1))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    ReduceByKey Transformation\n",
    "\n",
    "    Example: [(\"a\",1), (\"a\",1), (\"b\",1), (\"b\",1), (\"c\",1)] => [(\"a\",3),(\"b\",2), (\"c\",1)]\n",
    "\n",
    "\"\"\"\n",
    "counts = word_tuples.reduceByKey(lambda x1, x2: x1 + x2)\n",
    "\n",
    "\n",
    "# Print the result (10 records)\n",
    "counts.pprint()\n",
    "\n",
    "# Save to permanent storage\n",
    "#counts.transform(lambda rdd: rdd.coalesce(1)).saveAsTextFiles(\"/YOUR_PATH/output/wordCount\")\n",
    "\n",
    "# Start Spark Streaming\n",
    "ssc.start()\n",
    "\n",
    "# Await terminiation\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Create a test text source using the netcat tool. The netcat will set a listener to port 9999, and text typed in terminal will be read by Spark Streaming</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nc -lk 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Run the spark streaming application (above code)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark-submit --master local[2] /YOUR_PATH/spark_streaming_wordcount.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now you should have two terminal (one for messages and the other for spark streaming app). Enter random text messages in the terminal with netcat and look at the terminal with sparl streaming. How can you describe behaviour of the spark streaming app? What features do you notice? Why is it stateless?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:14pt; font-weight:bold\">2. Stateful Transformation</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To contents</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !!! COPY CONTENT OF THIS CELL AND PAST IT INTO SEPERATE PY FILE TO RUN IN TERMINAL !!!\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\n",
    "# Create Spark Context\n",
    "sc = SparkContext(appName=\"WordCountStatefull\")\n",
    "\n",
    "# Set log level\n",
    "#sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Batch interval (10 seconds)\n",
    "batch_interval = 10\n",
    "\n",
    "# Create Streaming Context\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "# Add checkpoint to preserve the states\n",
    "ssc.checkpoint(\"tmp_spark_streaming\") # == /user/cloudera/tmp_spark_streaming\n",
    "\n",
    "# Create a stream\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# TRANSFORMATION FOR EACH BATCH\n",
    "words = lines.flatMap(lambda line: line.split())\n",
    "word_tuples = words.map(lambda word: (word, 1))\n",
    "counts = word_tuples.reduceByKey(lambda x1, x2: x1 + x2)\n",
    "\n",
    "# function for updating values\n",
    "def update_total_count(currentCount, countState):\n",
    "    if countState is None:\n",
    "        countState = 0\n",
    "    return sum(currentCount, countState)\n",
    "\n",
    "# Update current values\n",
    "total_counts = counts.updateStateByKey(update_total_count)\n",
    "\n",
    "# Print the result (10 records)\n",
    "counts.pprint()\n",
    "#counts.transform(lambda rdd: rdd.coalesce(1)).saveAsTextFiles(\"/YOUR_PATH/output/wordCount\")\n",
    "\n",
    "# Start Spark Streaming\n",
    "ssc.start()\n",
    "\n",
    "# Await terminiation\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Run this spark streaming app in terminal as in the previous case. What is the difference between the results of the two applications?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:14pt; font-weight:bold\">3. Window Transformation</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To contents</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !!! COPY CONTENT OF THIS CELL AND PAST IT INTO SEPERATE PY FILE TO RUN IN TERMINAL !!!\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\n",
    "# Create Spark Context\n",
    "sc = SparkContext(appName=\"WordCountStatefullWindow\")\n",
    "\n",
    "# Set log level\n",
    "#sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Batch interval (10 seconds)\n",
    "batch_interval = 10\n",
    "\n",
    "# Create Streaming Context\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "# Add checkpoint to preserve the states\n",
    "ssc.checkpoint(\"tmp_spark_streaming\") # == /user/cloudera/tmp_spark_streaming\n",
    "\n",
    "# Create a stream\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "\n",
    "# TRANSFORMATION FOR EACH BATCH\n",
    "words = lines.flatMap(lambda line: line.split())\n",
    "word_tuples = words.map(lambda word: (word, 1))\n",
    "counts = word_tuples.reduceByKey(lambda x1, x2: x1 + x2)\n",
    "\n",
    "# Apply window\n",
    "windowed_word_counts = counts.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 20, 10)\n",
    "#windowed_word_counts = counts.reduceByKeyAndWindow(lambda x, y: x + y, None, 20, 10)\n",
    "\n",
    "\n",
    "# Print the result (10 records)\n",
    "windowed_word_counts.pprint()\n",
    "#windowed_word_counts.transform(lambda rdd: rdd.coalesce(1)).saveAsTextFiles(\"/YOUR_PATH/output/wordCount\")\n",
    "\n",
    "# Start Spark Streaming\n",
    "ssc.start()\n",
    "\n",
    "# Await terminiation\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:14pt; font-weight:bold\">4. Sources</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To contents</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://spark.apache.org/docs/latest/streaming-programming-guide.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
